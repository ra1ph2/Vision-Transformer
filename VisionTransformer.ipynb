{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "194YTo4VqBt4HXEGcBz__l49llDopmNMP",
      "authorship_tag": "ABX9TyNCZTp0j+sS0hGspyPCbLz6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7v2dPqfZFIV"
      },
      "source": [
        "#### Libraries Imported and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQIxfD9Ep0y"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7R221iUMI0"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc5EfTgZIBk"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75W0devEPY7"
      },
      "source": [
        "##### Vision Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention Module used to perform self-attention operation allowing the model to attend\n",
        "    information from different representation subspaces on an input sequence of embeddings.\n",
        "    The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Query, Key, Value -> ReshapeHeads -> Query.TransposedKey -> Softmax -> Dropout\n",
        "    -> AttentionScores.Value -> ReshapeHeadsBack -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        _reshape_heads(inp) :- \n",
        "        Changes the input sequence embeddings to reduced dimension according to the number\n",
        "        of attention heads to parallelize attention operation\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size * heads, seq_len, reduced_dim)\n",
        "\n",
        "        _reshape_heads_back(inp) :-\n",
        "        Changes the reduced dimension due to parallel attention heads back to the original\n",
        "        embedding size\n",
        "        (batch_size * heads, seq_len, reduced_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        forward(inp) :-\n",
        "        Performs the self-attention operation on the input sequence embedding.\n",
        "        Returns the output of self-attention as well as atttention scores\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim), (batch_size * heads, seq_len, seq_len)\n",
        "\n",
        "    Examples:\n",
        "        >>> attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        >>> out, weights = attention(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "        \n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYemII1gElcK"
      },
      "source": [
        "# Check if Dropout should be used after second Linear Layer\n",
        "class FeedForward(nn.Module):\n",
        "    '''\n",
        "    FeedForward Network with two sequential linear layers with GELU activation function\n",
        "    ,applied to the output of self attention operation. The sequence of operations is as\n",
        "    follows :-\n",
        "    \n",
        "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> FF = FeedForward(8, 1)\n",
        "        >>> out = FF(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.dropout(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrCi7DNGvCc"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''\n",
        "    Transformer Block combines both the attention module and the feed forward module with layer\n",
        "    normalization, dropout and residual connections. The sequence of operations is as follows :-\n",
        "    \n",
        "    Input -> LayerNorm1 -> Attention -> Residual -> LayerNorm2 -> FeedForward -> Output\n",
        "      |                                   |  |                                      |\n",
        "      |-------------Addition--------------|  |---------------Addition---------------|\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> TB = TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = TB(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        res = inp\n",
        "        out = self.norm1(inp)\n",
        "        out, _ = self.attention(out)\n",
        "        out = out + res\n",
        "        \n",
        "        res = out\n",
        "        out = self.norm2(out)\n",
        "        out = self.feed_forward(out)\n",
        "        out = out + res\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcBcReoPJ4NC"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    '''\n",
        "    Transformer combines multiple layers of Transformer Blocks in a sequential manner. The sequence\n",
        "    of the operations is as follows -\n",
        "\n",
        "    Input -> TB1 -> TB2 -> .......... -> TBn (n being the number of layers) -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, seq_len, embed_dim) -> (batch_size, seq_len, embed_dim)\n",
        "\n",
        "    Examples:\n",
        "        >>> transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        >>> out = transformer(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trans_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        out = inp\n",
        "        for block in self.trans_blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRN2k5-kLWYG"
      },
      "source": [
        "# Not Exactly Same as Paper\n",
        "class ClassificationHead(nn.Module):\n",
        "    '''\n",
        "    Classification Head attached to the first sequence token which is used as the arbitrary \n",
        "    classification token and used to optimize the transformer model by applying Cross-Entropy \n",
        "    loss. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> FC1 -> GELU -> Dropout -> FC2 -> Output\n",
        "\n",
        "    Args:\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classification classes in the dataset\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, embed_dim) -> (batch_size, classes)\n",
        "\n",
        "    Examples:\n",
        "        >>> CH = ClassificationHead(embed_dim, classes, dropout)\n",
        "        >>> out = CH(inp)\n",
        "    '''\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.softmax(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, classes) \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8nXI7gPxIs"
      },
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    '''\n",
        "    Vision Transformer is the complete end to end model architecture which combines all the above modules\n",
        "    in a sequential manner. The sequence of the operations is as follows -\n",
        "\n",
        "    Input -> CreatePatches -> ClassToken, PatchToEmbed , PositionEmbed -> Transformer -> ClassificationHead -> Output\n",
        "                                   |            | |                |\n",
        "                                   |---Concat---| |----Addition----|\n",
        "    \n",
        "    Args:\n",
        "        patch_size: Length of square patch size \n",
        "        max_len: Max length of learnable positional embedding\n",
        "        embed_dim: Dimension size of the hidden embedding\n",
        "        classes: Number of classes in the dataset\n",
        "        layers: Number of Transformer Blocks in the Transformer\n",
        "        channels: Number of channels in the input (Default=3)\n",
        "        heads: Number of parallel attention heads (Default=8)\n",
        "        activation: Optional activation function to be applied to the input while\n",
        "                    transforming to query, key and value matrixes (Default=None)\n",
        "        forward_expansion: The scale used to transform the input embedding to a higher dimension\n",
        "                           and then scaled back to capture richer information (Default=1)\n",
        "        dropout: Dropout value for the layer on attention_scores (Default=0.1)\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        It outputs the classification output as well as the sequence output of the transformer\n",
        "        (batch_size, channels, width, height) -> (batch_size, classes), (batch_size, seq_len+1, embed_dim)\n",
        "    \n",
        "    Examples:\n",
        "        >>> ViT = VisionTransformer(atch_size, max_len, embed_dim, classes, layers, channels, heads, activation, forward_expansion, dropout)\n",
        "        >>> class_out, hidden_seq = ViT(inp)\n",
        "    '''\n",
        "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, channels=3, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.name = 'VisionTransformer'\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.channels = channels\n",
        "        self.patch_to_embed = nn.Linear(patch_size * patch_size * channels, embed_dim)\n",
        "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
        "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, channels, width, height)\n",
        "        batch_size, channels, width, height = inp.size()\n",
        "        assert channels == self.channels\n",
        "\n",
        "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
        "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
        "        out = out.permute(0, 2, 3, 4, 1)\n",
        "        # out: (batch_size, seq_len, patch_size, patch_size, channels) | seq_len would be (width*height)/(patch_size**2)\n",
        "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
        "        \n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.patch_to_embed(out)\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat([class_token, out], dim=1)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "        position_embed = self.position_embed[:seq_len+1]\n",
        "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
        "        out = out + position_embed\n",
        "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
        "\n",
        "        out = self.transformer(out)\n",
        "        # out: (batch_size, seq_len+1, embed_dim) \n",
        "        class_token = out[:, 0]\n",
        "        # class_token: (batch_size, embed_dim)\n",
        "\n",
        "        class_out = self.classification_head(class_token)\n",
        "        # class_out: (batch_size, classes)\n",
        "        \n",
        "        return class_out, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJPR42t1S9li"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "class ResNetFeatures(nn.Module):\n",
        "    '''\n",
        "    ResNetFeatures outputs the lower level features from pretrained ResNet34 till the intial 5 layers \n",
        "    (conv1, bn1, relu, maxpool, layer1(3 conv layers)) to be used in the hybrid architecture to be \n",
        "    able to kickstart the learining faster. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> conv1 -> bn1 -> relu -> maxpool -> layer1 -> Output\n",
        "\n",
        "    Args:\n",
        "        No arguments required\n",
        "    \n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, 3, 224, 224) -> (batch_size, 64, 56, 56)\n",
        "    \n",
        "    Examples:\n",
        "        >>> resnet_features = ResNetFeatures()\n",
        "        >>> out = resnet_features(inp)\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ResNetFeatures, self).__init__()\n",
        "        layers = list(resnet34(pretrained=True).children())[:5] #all layer expect last layer\n",
        "        self.feature_extractor = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, 3, 224, 224)\n",
        "\n",
        "        out = self.feature_extractor(inp)\n",
        "\n",
        "        # out: (batch_size, 64, 56, 56)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsEV_yqAETZA"
      },
      "source": [
        "##### ResNet Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMIU5BMjmeD"
      },
      "source": [
        "class ResidualBlockSmall(nn.Module):\n",
        "    '''\n",
        "    ResidualBlockSmall implements the smaller block of the Residual Networks. It optionally also downsamples\n",
        "    the input according to the stride to match the output while adding the residual. The sequence of operations\n",
        "    is as follows :-\n",
        "\n",
        "    Input -> Conv1 -> BNorm1 -> ReLU -> Conv2 -> BNorm2 -> ReLU -> Output\n",
        "      |                                                              |\n",
        "      |-----------------Residual_Downsample (Optional)---------------|\n",
        "\n",
        "    Args:\n",
        "        input_channels: Number of input channels\n",
        "        out_channels: Number of output channels\n",
        "        residual_downsample: Residual Downsample dependent on if either height, width or channels change\n",
        "        stride: Stride value for the convolutional layers (Default=1)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, input_channels, height, width) -> (batch_size, out_channels, height, width)\n",
        "    \n",
        "    Examples:\n",
        "        >>> RBS = ResidualBlockSmall(input_channels, out_channels, residual_downsample, stride)\n",
        "        >>> out = RBS(inp)\n",
        "    '''\n",
        "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1):\n",
        "        super(ResidualBlockSmall, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.residual_downsample = residual_downsample\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        res = inp\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
        "        \n",
        "        if self.residual_downsample is not None:\n",
        "            res = self.residual_downsample(res)\n",
        "\n",
        "        out = self.activation(out + res)\n",
        "\n",
        "        # out: (batch_size, out_channels, height, width) | height, width depending on stride\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvBRaEPVktZd"
      },
      "source": [
        "class ResNetSmall(nn.Module):\n",
        "    '''\n",
        "    ResNetSmall consists of layers of the smaller residual block defined above (ResidualBlockSmall).\n",
        "    The layers are the residual blocks. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Conv1 -> BNorm1 -> ReLU -> MaxPool -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> AvgPool -> FC\n",
        "\n",
        "    Args:\n",
        "        layers: A four value array containing number of conv layers in each residual block\n",
        "        input_channels: number of input channels\n",
        "        classes: Number of classes in the dataset\n",
        "    \n",
        "    Methods:\n",
        "        _layer(num_layers (Number of conv layers)\n",
        "               ,input_channels (Number of input channels)\n",
        "               ,output_channels (Number of output channels)\n",
        "               ,stride (Stride value for conv layer)) :-\n",
        "        Returns the sequential wrapper with all the layers in the residual block constructed according\n",
        "        to the parameters.\n",
        "\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, input_channels, height, width) -> (batch_size, classes)\n",
        "\n",
        "    Examples:\n",
        "        >>> resnet = ResNetSmall(layers, input_channels, classes)\n",
        "        >>> out = resnet(inp)\n",
        "    '''\n",
        "    def __init__(self, layers, input_channels, classes):\n",
        "        super(ResNetSmall, self).__init__()\n",
        "        self.name = 'ResNet'\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bnorm1 = nn.BatchNorm2d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
        "        self.layer2 = self._layer(layers[1], input_channels=64, output_channels=128, stride=2)\n",
        "        self.layer3 = self._layer(layers[2], input_channels=128, output_channels=256, stride=2)\n",
        "        self.layer4 = self._layer(layers[3], input_channels=256, output_channels=512, stride=2)\n",
        "\n",
        "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avppool(out)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out\n",
        "\n",
        "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
        "        residual_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1:\n",
        "            residual_downsample = nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(output_channels * 4)\n",
        "            )\n",
        "        \n",
        "        layers.append(ResidualBlockSmall(input_channels, output_channels, residual_downsample, stride))\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(ResidualBlockSmall(output_channels, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQxpId2jEV_R"
      },
      "source": [
        "class ResidualBlockLarge(nn.Module):\n",
        "    '''\n",
        "    ResidualBlockLarge implements the larger block of the Residual Networks. It optionally also downsamples\n",
        "    the input according to the stride or output channels to match the output while adding the residual. The \n",
        "    sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Conv1 -> BNorm1 -> ReLU -> Conv2 -> BNorm2 -> ReLU -> Conv3 -> BNorm3 -> ReLU -> Output\n",
        "      |                                                                                          |\n",
        "      |-----------------------------Residual_Downsample (Optional)-------------------------------|\n",
        "\n",
        "    Args:\n",
        "        input_channels: Number of input channels\n",
        "        out_channels: Number of output channels\n",
        "        residual_downsample: Residual Downsample dependent on if either height, width or channels change\n",
        "        stride: Stride value for the convolutional layers (Default=1)\n",
        "        expansion: Expansion of the input channels during convolutions (Default=4)\n",
        "\n",
        "    Methods:\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, input_channels, height, width) -> (batch_size, out_channels * expansion, height, width)\n",
        "    \n",
        "    Examples:\n",
        "        >>> RBL = ResidualBlockLarge(input_channels, out_channels, residual_downsample, stride, expansion)\n",
        "        >>> out = RBL(inp)\n",
        "    '''\n",
        "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1, expansion=4):\n",
        "        super(ResidualBlockLarge, self).__init__()\n",
        "        self.expansion = expansion\n",
        "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm3 = nn.BatchNorm2d(out_channels * expansion)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.residual_downsample = residual_downsample\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        res = inp\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
        "        out = self.activation(self.bnorm3(self.conv3(out)))\n",
        "        \n",
        "        if self.residual_downsample is not None:\n",
        "            res = self.residual_downsample(res)\n",
        "\n",
        "        out = self.activation(out + res)\n",
        "\n",
        "        # out: (batch_size, out_channels * expansion, height, width) | height, width depending on stride\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ss--kZkSGNU"
      },
      "source": [
        "class ResNetLarge(nn.Module):\n",
        "    '''\n",
        "    ResNetLarge consists of layers of the larger residual block defined above (ResidualBlockLarger).\n",
        "    The layers are the residual blocks. The sequence of operations is as follows :-\n",
        "\n",
        "    Input -> Conv1 -> BNorm1 -> ReLU -> MaxPool -> Layer1 -> Layer2 -> Layer3 -> Layer4 -> AvgPool -> FC\n",
        "\n",
        "    Args:\n",
        "        layers: A four value array containing number of conv layers in each residual block\n",
        "        input_channels: number of input channels\n",
        "        classes: Number of classes in the dataset\n",
        "    \n",
        "    Methods:\n",
        "        _layer(num_layers (Number of conv layers)\n",
        "               ,input_channels (Number of input channels)\n",
        "               ,output_channels (Number of output channels)\n",
        "               ,stride (Stride value for conv layer)) :-\n",
        "        Returns the sequential wrapper with all the layers in the residual block constructed according\n",
        "        to the parameters.\n",
        "\n",
        "        forward(inp) :-\n",
        "        Applies the sequence of operations mentioned above.\n",
        "        (batch_size, input_channels, height, width) -> (batch_size, classes)\n",
        "\n",
        "    Examples:\n",
        "        >>> resnet = ResNetLarge(layers, input_channels, classes)\n",
        "        >>> out = resnet(inp)\n",
        "    '''\n",
        "    def __init__(self, layers, input_channels, classes):\n",
        "        super(ResNetLarge, self).__init__()\n",
        "        self.name = 'ResNet'\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bnorm1 = nn.BatchNorm2d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
        "        self.layer2 = self._layer(layers[1], input_channels=256, output_channels=128, stride=2)\n",
        "        self.layer3 = self._layer(layers[2], input_channels=512, output_channels=256, stride=2)\n",
        "        self.layer4 = self._layer(layers[3], input_channels=1024, output_channels=512, stride=2)\n",
        "\n",
        "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avppool(out)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out\n",
        "\n",
        "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
        "        residual_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # Checks if there would be potential mismatch in any of height, width or channels between input and output. \n",
        "        # 4 is the value of the expansion for large ResNets\n",
        "        if stride != 1 or input_channels != output_channels * 4:\n",
        "            residual_downsample = nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels * 4, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(output_channels * 4)\n",
        "            )\n",
        "        \n",
        "        layers.append(ResidualBlockLarge(input_channels, output_channels, residual_downsample, stride))\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(ResidualBlockLarge(output_channels * 4, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4piMb4xwl3MX"
      },
      "source": [
        "def ResNet34(input_channels, classes):\n",
        "    '''\n",
        "    Initalization of ResNet34 using the layers as mentioned in the paper and using ResNetSmall module.\n",
        "\n",
        "    Args:\n",
        "        input_channels: Number of input channels\n",
        "        classes: Number of classes in the dataset\n",
        "    \n",
        "    Output:\n",
        "        ResNetSmall Object\n",
        "    '''\n",
        "    return ResNetSmall([3, 4, 6, 3], input_channels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeceF_RUaVO4"
      },
      "source": [
        "def ResNet50(input_channels, classes):\n",
        "    '''\n",
        "    Initalization of ResNet50 using the layers as mentioned in the paper and using ResNetLarge module.\n",
        "    \n",
        "    Args:\n",
        "        input_channels: Number of input channels\n",
        "        classes: Number of classes in the dataset\n",
        "    \n",
        "    Output:\n",
        "        ResNetLarge Object\n",
        "    '''\n",
        "    return ResNetLarge([3, 4, 6, 3], input_channels, classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGS4bmCRWmIg"
      },
      "source": [
        "#### Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60fXexWWWlpJ"
      },
      "source": [
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
        "    '''\n",
        "    A wrapper function that creates a DataLoader for CIFAR100 dataset loaded from torchvision using \n",
        "    the parameters supplied and applies the required data augmentations.\n",
        "\n",
        "    Args:\n",
        "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
        "        batch_size: Batch size to used for loading data (Default=8)\n",
        "        num_workers: Number of parallel workers used to load data (Default=2)\n",
        "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
        "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
        "        normalize: A string to decide the normalization to applied to the input images\n",
        "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
        "    \n",
        "    Output:\n",
        "        DataLoader Object\n",
        "    '''\n",
        "    if normalize == 'imagenet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif normalize == 'standard':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std =  [0.5, 0.5, 0.5]\n",
        "\n",
        "    if split == 'train':\n",
        "        if size == '224':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop((224,224), scale=(0.8, 1.0)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        elif size == '32':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(15),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        \n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        if size == '224':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        elif size == '32':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtYCa72d0RJ6"
      },
      "source": [
        "def CIFAR10DataLoader(split, batch_size=8, num_workers=2, shuffle=True, size='32', normalize='standard'):\n",
        "    '''\n",
        "    A wrapper function that creates a DataLoader for CIFAR10 dataset loaded from torchvision using \n",
        "    the parameters supplied and applies the required data augmentations.\n",
        "\n",
        "    Args:\n",
        "        split: A string to decide if train or test data to be used (Values: 'train', 'test')\n",
        "        batch_size: Batch size to used for loading data (Default=8)\n",
        "        num_workers: Number of parallel workers used to load data (Default=2)\n",
        "        shuffle: Boolean value to decide if data should be randomized (Default=True)\n",
        "        size: A string to decide the size of the input images (Default='32') (Values: '32','224')\n",
        "        normalize: A string to decide the normalization to applied to the input images\n",
        "                   (Default='standard') (Values: 'standard', 'imagenet')\n",
        "    \n",
        "    Output:\n",
        "        DataLoader Object\n",
        "    '''\n",
        "    if normalize == 'imagenet':\n",
        "        mean = [0.485, 0.456, 0.406]\n",
        "        std = [0.229, 0.224, 0.225]\n",
        "    elif normalize == 'standard':\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std =  [0.5, 0.5, 0.5]\n",
        "\n",
        "    if split == 'train':\n",
        "        if size == '224':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.RandomResizedCrop((224,224), scale=(0.5, 1.0)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        elif size == '32':\n",
        "            train_transform = transforms.Compose([\n",
        "                transforms.Resize((48, 48)),\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomRotation(15),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        \n",
        "        cifar10 = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar10, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        if size == '224':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "        elif size == '32':\n",
        "            test_transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean, std)\n",
        "            ])\n",
        "\n",
        "        cifar10 = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar10, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_YjSG0aLJR"
      },
      "source": [
        "#### Training and Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byfz7zrEaKxq"
      },
      "source": [
        "# Initializations of all the constants used in the training and testing process\n",
        "\n",
        "lr = 0.003\n",
        "batch_size = 256\n",
        "num_workers = 2\n",
        "shuffle = True\n",
        "patch_size = 4\n",
        "image_sz = 32\n",
        "max_len = 100 # All sequences must be less than 1000 including class token\n",
        "embed_dim = 512\n",
        "classes = 10\n",
        "layers = 12\n",
        "channels = 3\n",
        "resnet_features_channels = 64\n",
        "heads = 16\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89pJGwZaUBs"
      },
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler, resnet_features=None):\n",
        "    '''\n",
        "    Function used to train the model over a single epoch and update it according to the\n",
        "    calculated gradients.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        optimizer: Optimizer used update the model\n",
        "        scheduler: Scheduler used to update the learing rate for faster convergence \n",
        "                   (Commented out due to poor results)\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "\n",
        "    Output:\n",
        "        running_loss: Training Loss (Float)\n",
        "        running_accuracy: Training Accuracy (Float)\n",
        "    '''\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for data, target in tqdm(dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if model.name == 'VisionTransformer':\n",
        "            with torch.no_grad():\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "            output, _ = model(data)\n",
        "        elif model.name == 'ResNet':\n",
        "            output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(dataloader)\n",
        "        running_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return running_loss, running_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXAFKy14EIyU"
      },
      "source": [
        "def evaluation(model, dataloader, criterion, resnet_features=None):\n",
        "    '''\n",
        "    Function used to evaluate the model on the test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Model supplied to the function\n",
        "        dataloader: DataLoader supplied to the function\n",
        "        criterion: Criterion used to calculate loss\n",
        "        resnet_features: Model to get Resnet Features for the hybrid architecture (Default=None)\n",
        "    \n",
        "    Output:\n",
        "        test_loss: Testing Loss (Float)\n",
        "        test_accuracy: Testing Accuracy (Float)\n",
        "    '''\n",
        "    with torch.no_grad():\n",
        "        test_accuracy = 0.0\n",
        "        test_loss = 0.0\n",
        "        for data, target in tqdm(dataloader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if model.name == 'VisionTransformer':\n",
        "                if resnet_features != None:\n",
        "                    data = resnet_features(data)\n",
        "                output, _ = model(data)\n",
        "            elif model.name == 'ResNet':\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc = (output.argmax(dim=1) == target).float().mean()\n",
        "            test_accuracy += acc / len(dataloader)\n",
        "            test_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1NNc-FxvJU7"
      },
      "source": [
        "#### Model Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94FIkGknvMUW"
      },
      "source": [
        "Run either one the following subcells according to the models selected to train and test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAkCvMzfN3R"
      },
      "source": [
        "##### Model - Vision Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVBkOJnnvWzH"
      },
      "source": [
        "Recommended Values for the following Architecture\n",
        "\n",
        "- patch_size = 4\n",
        "- max_len = 100\n",
        "- embed_dim = 512\n",
        "- classes = According to Dataset\n",
        "- layers = 12\n",
        "- channels = 3\n",
        "- heads = 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_wT7pXe5q9"
      },
      "source": [
        "# Vision Transformer Architecture\n",
        "\n",
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    channels=channels,\n",
        "    heads=heads).to(device)\n",
        "\n",
        "resnet_features = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AtT5cLKvtxC"
      },
      "source": [
        "Recommended Values for the following Architecture\n",
        "\n",
        "- patch_size = 7\n",
        "- max_len = 100\n",
        "- embed_dim = 512\n",
        "- classes = According to Dataset\n",
        "- layers = 12\n",
        "- channels = 64 (Resnet Features Channels)\n",
        "- heads = 16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrWmtB8QVT50"
      },
      "source": [
        "# Hybrid Vision Transformer Architecture\n",
        "\n",
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    channels=resnet_features_channels,\n",
        "    heads=heads).to(device)\n",
        "\n",
        "resnet_features = ResNetFeatures().to(device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq_fd2gjfRqD"
      },
      "source": [
        "##### Model - ResNet50 or ResNet34"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MduGlJkPwEmp"
      },
      "source": [
        "Recommended Values for the following Architecture\n",
        "\n",
        "- input_channels = 3\n",
        "- classes = According to Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJhrjCYhmjII"
      },
      "source": [
        "# ResNet34 Architecture\n",
        "\n",
        "model = ResNet34(\n",
        "    input_channels=3,\n",
        "    classes=classes).to(device)\n",
        "\n",
        "resnet_features = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQzAinFRwMcw"
      },
      "source": [
        "Recommended Values for the following Architecture\n",
        "\n",
        "- input_channels = 3\n",
        "- classes = According to Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztkBOLRe6pO"
      },
      "source": [
        "# ResNet50 Architecture\n",
        "\n",
        "model = ResNet50(\n",
        "    input_channels=3,\n",
        "    classes=classes).to(device)\n",
        "\n",
        "resnet_features = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0x9qE4HhUhR"
      },
      "source": [
        "#### Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPVinX5pw46Q"
      },
      "source": [
        "##### CIFAR10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA4SFPwTwxcf"
      },
      "source": [
        "train_dataloader = CIFAR10DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, size='32', normalize='standard')\n",
        "test_dataloader = CIFAR10DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False, size='32', normalize='standard')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    train_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "    if (epoch+1)%5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_acc': train_accs,\n",
        "            'test_acc': test_accs\n",
        "        }, './drive/MyDrive/VisionTransformer/' + model.name + '_CIFAR10_checkpoint.pt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7yDEtyzw8oU"
      },
      "source": [
        "##### CIFAR100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp1l68Z_UiGc"
      },
      "source": [
        "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle, size='32', normalize='standard')\n",
        "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False, size='32', normalize='standard')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler, resnet_features)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    train_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion, resnet_features)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "    if (epoch+1)%5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_acc': train_accs,\n",
        "            'test_acc': test_accs\n",
        "        }, './drive/MyDrive/VisionTransformer/' + model.name + '_CIFAR100_checkpoint.pt') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KjgBoZoxU42"
      },
      "source": [
        "##### Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsB1V8Gl0jnQ"
      },
      "source": [
        "train_accs = [acc.cpu().item() for acc in train_accs]\n",
        "test_accs = [acc.cpu().item() for acc in test_accs]\n",
        "# print(train_accs)\n",
        "# print(test_accs)\n",
        "plt.style.use('seaborn')\n",
        "plt.plot(range(1, 101), train_accs, label='Train Accuracy')\n",
        "plt.plot(range(1, 101), test_accs, label='Test Accuracy')\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.title(\"Train vs Test Accuracy\")\n",
        "plt.legend(loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}