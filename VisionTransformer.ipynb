{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "mount_file_id": "194YTo4VqBt4HXEGcBz__l49llDopmNMP",
      "authorship_tag": "ABX9TyNwOCbbjwkL8q2lvSx2pzY/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7v2dPqfZFIV"
      },
      "source": [
        "#### Libraries Imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQIxfD9Ep0y"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7R221iUMI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a1e000-0bbf-4d09-82c9-9de7cc07ecdd"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU:  Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc5EfTgZIBk"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75W0devEPY7"
      },
      "source": [
        "##### Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "        \n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOGwfjHJQR_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70df1a71-6783-48d8-dfc0-f809e0e3e3a1"
      },
      "source": [
        "attention = Attention(embed_dim=4, heads=2, activation=None)\n",
        "attention_lib = nn.MultiheadAttention(embed_dim=4, num_heads=2)\n",
        "inp = torch.ones((1, 2, 4))\n",
        "print(inp)\n",
        "out, wts = attention(inp)\n",
        "print(out)\n",
        "print(wts)\n",
        "out_lib, out_wts = attention_lib(inp.permute(1, 0, 2), inp.permute(1, 0, 2), inp.permute(1, 0, 2))\n",
        "print(out_lib.permute(1, 0, 2))\n",
        "print(out_wts)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[-1.3208,  0.5635,  0.5086,  0.4329],\n",
            "         [-1.3208,  0.5635,  0.5086,  0.4329]]], grad_fn=<UnsafeViewBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]],\n",
            "\n",
            "        [[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[[ 0.1708, -0.1359,  0.3290,  0.5390],\n",
            "         [ 0.1708, -0.1359,  0.3290,  0.5390]]], grad_fn=<PermuteBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYemII1gElcK"
      },
      "source": [
        "# Check if Dropout should be used after second Linear Layer\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.dropout(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1r7Cak2F9CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b388b2-f2e8-4bcf-b471-165699961fcc"
      },
      "source": [
        "ff = FeedForward(8, 1)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = ff(inp)\n",
        "print(out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[ 0.1146, -0.0022, -0.3961,  0.1302,  0.0784, -0.1796,  0.1054,\n",
            "          -0.5608],\n",
            "         [ 0.1146, -0.0022, -0.3961,  0.1302,  0.0784, -0.1796,  0.1054,\n",
            "          -0.5608]]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrCi7DNGvCc"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        res = inp\n",
        "        out = self.norm1(inp)\n",
        "        out, _ = self.attention(out)\n",
        "        out = out + res\n",
        "        \n",
        "        res = out\n",
        "        out = self.norm2(out)\n",
        "        out = self.feed_forward(out)\n",
        "        out = out + res\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IomPGN9UJj1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1c135b-212c-4f07-cfb9-3af2eb773efa"
      },
      "source": [
        "tb = TransformerBlock(8, 2)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = tb(inp)\n",
        "print(out)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[0.8445, 0.3871, 0.5214, 1.4567, 0.4432, 0.3485, 1.1854, 1.3802],\n",
            "         [0.7812, 0.2160, 0.3708, 1.7873, 0.3957, 0.3838, 1.1758, 1.3659]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcBcReoPJ4NC"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trans_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        out = inp\n",
        "        for block in self.trans_blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfxmE-mJLILR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fa93640-e826-43ab-9da4-a937fd37bf17"
      },
      "source": [
        "tb = TransformerBlock(8, 2)\n",
        "trans = Transformer(8, 1, 2)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = tb(inp)\n",
        "print(out)\n",
        "out = trans(inp)\n",
        "print(out)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[1.0180, 0.9622, 1.3358, 0.4052, 0.4615, 0.7469, 1.0573, 0.9045],\n",
            "         [0.8306, 0.8675, 1.1718, 0.5876, 0.3138, 0.6845, 1.0656, 0.8719]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[[1.2446, 0.9350, 1.1241, 0.5928, 0.5922, 1.1145, 0.8107, 1.2146],\n",
            "         [1.2689, 1.1372, 1.1191, 0.3576, 0.8591, 1.1977, 0.8784, 1.2873]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRN2k5-kLWYG"
      },
      "source": [
        "# Not Exactly Same as Paper | Check if Dropout should be used here\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.softmax(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, embed_dim) | SoftMaxed along the last dimension\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfBFnhBhOFIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edbbf54-48c1-4b30-fd89-b1e2868f814f"
      },
      "source": [
        "ch = ClassificationHead(8, 2)\n",
        "inp = torch.ones((2, 8))\n",
        "print(inp)\n",
        "out = ch(inp)\n",
        "print(out)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[-0.2210, -0.4804],\n",
            "        [-0.1698, -0.4597]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8nXI7gPxIs"
      },
      "source": [
        "#TODO: Remove to.device from class_token\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.name = 'VisionTransformer'\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_to_embed = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
        "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, 3, width, height)\n",
        "        batch_size, channels, width, height = inp.size()\n",
        "        assert channels == 3\n",
        "\n",
        "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
        "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
        "        out = out.permute(0, 2, 3, 4, 1)\n",
        "        # out: (batch_size, seq_len, patch_size, patch_size, 3) | seq_len would be (width*height)/(patch_size**2)\n",
        "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
        "        \n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.patch_to_embed(out)\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat([class_token, out], dim=1)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "        position_embed = self.position_embed[:seq_len+1]\n",
        "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
        "        out = out + position_embed\n",
        "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
        "\n",
        "        out = self.transformer(out)\n",
        "        # out: (batch_size, seq_len+1, embed_dim) \n",
        "        class_token = out[:, 0]\n",
        "        # class_token: (batch_size, embed_dim)\n",
        "\n",
        "        class_out = self.classification_head(class_token)\n",
        "        # class_out: (batch_size, classes)\n",
        "        \n",
        "        return class_out, out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W82aFCPPWIus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb2e54bb-c6d7-4636-aca3-99c452cc4e47"
      },
      "source": [
        "vit = VisionTransformer(2, 3, 8, 2, 2).to(device)\n",
        "inp = torch.ones((2, 3, 2, 2)).to(device)\n",
        "print(inp)\n",
        "class_out, out = vit(inp)\n",
        "print(class_out)\n",
        "print(class_out.shape)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]]], device='cuda:0')\n",
            "tensor([[0.8111, 0.9749],\n",
            "        [0.6231, 0.7716]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([2, 2])\n",
            "tensor([[[-0.8753, -0.2290, -0.6732, -0.3413,  0.8313, -0.8553, -1.3122,\n",
            "           2.3801],\n",
            "         [ 0.0847,  0.1271, -2.5542, -0.3073, -0.6500, -0.9483, -2.6019,\n",
            "          -0.4426]],\n",
            "\n",
            "        [[-1.3445, -0.2853, -0.5775,  0.1917,  0.8798, -1.0493, -1.2134,\n",
            "           2.2441],\n",
            "         [-0.4474,  0.2314, -2.2715, -0.0509, -0.6878, -1.3577, -2.4800,\n",
            "          -0.5389]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsEV_yqAETZA"
      },
      "source": [
        "##### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUMIU5BMjmeD"
      },
      "source": [
        "class ResidualBlockSmall(nn.Module):\n",
        "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1):\n",
        "        super(ResidualBlockSmall, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.residual_downsample = residual_downsample\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        res = inp\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
        "        \n",
        "        if self.residual_downsample is not None:\n",
        "            res = self.residual_downsample(res)\n",
        "\n",
        "        out = self.activation(out + res)\n",
        "\n",
        "        # out: (batch_size, out_channels * expansion, height, width) | height, width depending on stride\n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvBRaEPVktZd"
      },
      "source": [
        "class ResNetSmall(nn.Module):\n",
        "    def __init__(self, layers, input_channels, classes):\n",
        "        super(ResNetSmall, self).__init__()\n",
        "        self.name = 'ResNet'\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bnorm1 = nn.BatchNorm2d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
        "        self.layer2 = self._layer(layers[1], input_channels=64, output_channels=128, stride=2)\n",
        "        self.layer3 = self._layer(layers[2], input_channels=128, output_channels=256, stride=2)\n",
        "        self.layer4 = self._layer(layers[3], input_channels=256, output_channels=512, stride=2)\n",
        "\n",
        "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avppool(out)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out\n",
        "\n",
        "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
        "        residual_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        if stride != 1:\n",
        "            residual_downsample = nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(output_channels * 4)\n",
        "            )\n",
        "        \n",
        "        layers.append(ResidualBlockSmall(input_channels, output_channels, residual_downsample, stride))\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(ResidualBlockSmall(output_channels, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQxpId2jEV_R"
      },
      "source": [
        "class ResidualBlockLarge(nn.Module):\n",
        "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1, expansion=4):\n",
        "        super(ResidualBlockLarge, self).__init__()\n",
        "        self.expansion = expansion\n",
        "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm3 = nn.BatchNorm2d(out_channels * expansion)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.residual_downsample = residual_downsample\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        res = inp\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
        "        out = self.activation(self.bnorm3(self.conv3(out)))\n",
        "        \n",
        "        if self.residual_downsample is not None:\n",
        "            res = self.residual_downsample(res)\n",
        "\n",
        "        out = self.activation(out + res)\n",
        "\n",
        "        # out: (batch_size, out_channels * expansion, height, width) | height, width depending on stride\n",
        "        return out"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ss--kZkSGNU"
      },
      "source": [
        "class ResNetLarge(nn.Module):\n",
        "    def __init__(self, layers, input_channels, classes):\n",
        "        super(ResNetLarge, self).__init__()\n",
        "        self.name = 'ResNet'\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bnorm1 = nn.BatchNorm2d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
        "        self.layer2 = self._layer(layers[1], input_channels=256, output_channels=128, stride=2)\n",
        "        self.layer3 = self._layer(layers[2], input_channels=512, output_channels=256, stride=2)\n",
        "        self.layer4 = self._layer(layers[3], input_channels=1024, output_channels=512, stride=2)\n",
        "\n",
        "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avppool(out)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out\n",
        "\n",
        "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
        "        residual_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # 4 is the value of the expansion for large ResNets\n",
        "        if stride != 1 or input_channels != output_channels * 4:\n",
        "            residual_downsample = nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels * 4, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(output_channels * 4)\n",
        "            )\n",
        "        \n",
        "        layers.append(ResidualBlockLarge(input_channels, output_channels, residual_downsample, stride))\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(ResidualBlockLarge(output_channels * 4, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4piMb4xwl3MX"
      },
      "source": [
        "def ResNet34(input_channels, classes):\n",
        "    return ResNetSmall([3, 4, 6, 3], input_channels, classes)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeceF_RUaVO4"
      },
      "source": [
        "def ResNet50(input_channels, classes):\n",
        "    return ResNetLarge([3, 4, 6, 3], input_channels, classes)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGS4bmCRWmIg"
      },
      "source": [
        "#### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60fXexWWWlpJ"
      },
      "source": [
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True):\n",
        "\n",
        "    CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "    \n",
        "    mean = CIFAR100_TRAIN_MEAN\n",
        "    std = CIFAR100_TRAIN_STD\n",
        "\n",
        "    if split == 'train':\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop((224,224), scale=(0.5, 1.0)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        # train_transform = transforms.Compose([\n",
        "        #     transforms.RandomCrop(32, padding=4),\n",
        "        #     transforms.RandomHorizontalFlip(),\n",
        "        #     transforms.RandomRotation(15),\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Normalize(mean, std)\n",
        "        # ])\n",
        "        \n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        # test_transform = transforms.Compose([\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Normalize(mean, std)\n",
        "        # ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_YjSG0aLJR"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byfz7zrEaKxq"
      },
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "shuffle = True\n",
        "patch_size = 16\n",
        "image_sz = 224\n",
        "max_len = ((image_sz//patch_size) * (image_sz//patch_size)) + 1 # +1 for the class token\n",
        "embed_dim = 512\n",
        "classes = 100\n",
        "layers = 6\n",
        "heads = 8\n",
        "epochs = 100"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89pJGwZaUBs"
      },
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for data, target in tqdm(dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if model.name == 'VisionTransformer':\n",
        "            output, _ = model(data)\n",
        "        elif model.name == 'ResNet':\n",
        "            output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(dataloader)\n",
        "        running_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return running_loss, running_accuracy"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXAFKy14EIyU"
      },
      "source": [
        "def evaluation(model, dataloader, criterion):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_accuracy = 0.0\n",
        "        test_loss = 0.0\n",
        "        for data, target in tqdm(dataloader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if model.name == 'VisionTransformer':\n",
        "                output, _ = model(data)\n",
        "            elif model.name == 'ResNet':\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc = (output.argmax(dim=1) == target).float().mean()\n",
        "            test_accuracy += acc / len(dataloader)\n",
        "            test_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAkCvMzfN3R"
      },
      "source": [
        "##### Model - Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_wT7pXe5q9"
      },
      "source": [
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    heads=heads).to(device)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq_fd2gjfRqD"
      },
      "source": [
        "##### Model - ResNet50 or ResNet34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJhrjCYhmjII"
      },
      "source": [
        "model = ResNet34(\n",
        "    input_channels=3,\n",
        "    classes=classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztkBOLRe6pO"
      },
      "source": [
        "model = ResNet50(\n",
        "    input_channels=3,\n",
        "    classes=classes).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0x9qE4HhUhR"
      },
      "source": [
        "##### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQel00aDWi4r",
        "outputId": "ab6620f9-5e3a-4009-a200-54ee8602fd98"
      },
      "source": [
        "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    train_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = evaluation(model, test_dataloader, criterion)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)\n",
        "\n",
        "    if (epoch+1)%5 == 0:\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model': model,\n",
        "            'optimizer': optimizer,\n",
        "            'scheduler': scheduler,\n",
        "            'train_acc': train_accs,\n",
        "            'test_acc': test_accs\n",
        "        }, './drive/MyDrive/VisionTransformer/checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [08:10<00:00,  1.25s/it]\n",
            "  0%|          | 0/79 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 - acc: 0.0679 - loss : 4.1917\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:36<00:00,  2.14it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test acc: 0.1060 - test loss : 3.8955\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 391/391 [08:09<00:00,  1.25s/it]\n",
            "  0%|          | 0/79 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 2 - acc: 0.1258 - loss : 3.7785\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:36<00:00,  2.14it/s]\n",
            "  0%|          | 0/391 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test acc: 0.1469 - test loss : 3.6356\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 4/391 [00:05<09:23,  1.46s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}