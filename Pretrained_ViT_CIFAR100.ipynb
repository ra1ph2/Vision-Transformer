{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pretrained ViT CIFAR100",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17a5d13a770b4334865005366206befd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14a6b9bb45ca44d5ac8b7be4624dec8d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_549b1a11f1c14e8497520a60c48f7d34",
              "IPY_MODEL_d3ad748271e841ce82aa1c09bc238c04"
            ]
          }
        },
        "14a6b9bb45ca44d5ac8b7be4624dec8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "549b1a11f1c14e8497520a60c48f7d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3832748f1da34242b3d0152679dc8fc8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_857ac83b74cd4ad9afda4241d9d52014"
          }
        },
        "d3ad748271e841ce82aa1c09bc238c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_437701e6abae497d89a9e309a3b48c0b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 169009152/? [00:20&lt;00:00, 53514591.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b0c13fc7fd64c169e2598f23d7d84ba"
          }
        },
        "3832748f1da34242b3d0152679dc8fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "857ac83b74cd4ad9afda4241d9d52014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "437701e6abae497d89a9e309a3b48c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b0c13fc7fd64c169e2598f23d7d84ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PFl0qKO2woV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "989182ab-de5c-4aac-d1d2-5dd8fac2f5a0"
      },
      "source": [
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-18 07:37:35--  https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.20.128, 74.125.195.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.20.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412815506 (394M) [application/octet-stream]\n",
            "Saving to: â€˜ViT-B_16.npzâ€™\n",
            "\n",
            "ViT-B_16.npz        100%[===================>] 393.69M  59.3MB/s    in 6.6s    \n",
            "\n",
            "2020-11-18 07:37:42 (59.3 MB/s) - â€˜ViT-B_16.npzâ€™ saved [412815506/412815506]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PIWToa1_vaS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db15b53-40d1-487b-bc9f-d40b01deb352"
      },
      "source": [
        "!pip install ml_collections-0.1.0-py3-none-any.whl"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./ml_collections-0.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from ml-collections==0.1.0) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from ml-collections==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from ml-collections==0.1.0) (0.5.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from ml-collections==0.1.0) (0.10.0)\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUM2FYFw24nD"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.datasets import cifar100\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, BatchNormalization, Flatten, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import albumentations as albu\n",
        "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import typing\n",
        "import io\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# coding=utf-8\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ2ePaih_mZA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9e02f8-b730-4c87-fd06-c41a02dbd4fc"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU:  Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4kwdIbd7DZh"
      },
      "source": [
        "import ml_collections\n",
        "\n",
        "\n",
        "def get_testing():\n",
        "    \"\"\"Returns a minimal configuration for testing.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 1\n",
        "    config.transformer.num_heads = 1\n",
        "    config.transformer.num_layers = 1\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 768\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b32_config():\n",
        "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l16_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1024\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 4096\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 24\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l32_config():\n",
        "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
        "    config = get_l16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_h14_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n",
        "    config.hidden_size = 1280\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 5120\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 32\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZpXty1yAPrp"
      },
      "source": [
        "\n",
        "\n",
        "#import models.configs as configs\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if weights.ndim == 4:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        img_size = _pair(img_size)\n",
        "        patch_size = _pair(config.patches[\"size\"])\n",
        "        n_patches = (img_size[0]//patch_size[0]) * (img_size[1]//patch_size[1])\n",
        "\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=100, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "\n",
        "        #if labels is not None:\n",
        "        #    loss_fct = CrossEntropyLoss()\n",
        "        #    loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "        #    return loss\n",
        "        #else:\n",
        "        return x\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"]))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "\n",
        "CONFIGS = {\n",
        "    'ViT-B_16': get_b16_config(),\n",
        "    'ViT-B_32': get_b32_config(),\n",
        "    'ViT-L_16': get_l16_config(),\n",
        "    'ViT-L_32': get_l32_config(),\n",
        "    'ViT-H_14': get_h14_config(),\n",
        "    'testing': get_testing(),\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re0hO849CExA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520a3879-5c77-4e59-8af1-81b4b5bf8cb0"
      },
      "source": [
        "config = CONFIGS[\"ViT-B_16\"]\n",
        "model = VisionTransformer(config, img_size=224, num_classes=21843, zero_head=False, vis=True)\n",
        "model.load_from(np.load(\"/content/ViT-B_16.npz\"))\n",
        "model.to(device)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (transformer): Transformer(\n",
              "    (embeddings): Embeddings(\n",
              "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (1): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (2): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (3): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (4): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (5): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (6): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (7): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (8): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (9): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (10): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "        (11): Block(\n",
              "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (ffn): Mlp(\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (attn): Attention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (softmax): Softmax(dim=-1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (head): Linear(in_features=768, out_features=21843, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4GQ0B2pDYYt"
      },
      "source": [
        "# Not Exactly Same as Paper | Check if Dropout should be used here\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2).to(device)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes).to(device)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        out = self.softmax(self.fc2(out))\n",
        "\n",
        "        # out: (batch_size, embed_dim) | SoftMaxed along the last dimension\n",
        "        return out\n",
        "classhead = ClassificationHead(768,100)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSu56uDbE4-j"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True):\n",
        "\n",
        "    CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "    \n",
        "    mean = CIFAR100_TRAIN_MEAN\n",
        "    std = CIFAR100_TRAIN_STD\n",
        "\n",
        "    if split == 'train':\n",
        "        #train_transform = transforms.Compose([\n",
        "            #transforms.Resize((224,224)),  \n",
        "        #    transforms.RandomCrop(32, padding=4),\n",
        "        #    transforms.RandomHorizontalFlip(),\n",
        "        #    transforms.RandomRotation(15),\n",
        "        #    transforms.ToTensor(),\n",
        "        #    transforms.Normalize(mean, std)\n",
        "        #])\n",
        "        train_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop((224,224), scale=(0.05, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "        \n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teKIbNOM6GLM"
      },
      "source": [
        "lr = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_workers = 4\n",
        "shuffle = True\n",
        "#patch_size = 4\n",
        "#max_len = ((32//patch_size) * (32//patch_size)) + 1 # +1 for the class token\n",
        "#embed_dim = 128\n",
        "classes = 100\n",
        "#layers = 6\n",
        "#heads = 8\n",
        "epochs = 100\n",
        "decay_type = 'cosine'\n",
        "warmup_steps = 20\n",
        "t_total = epochs\n",
        "num_steps = epochs"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvXOjfCiA8e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "17a5d13a770b4334865005366206befd",
            "14a6b9bb45ca44d5ac8b7be4624dec8d",
            "549b1a11f1c14e8497520a60c48f7d34",
            "d3ad748271e841ce82aa1c09bc238c04",
            "3832748f1da34242b3d0152679dc8fc8",
            "857ac83b74cd4ad9afda4241d9d52014",
            "437701e6abae497d89a9e309a3b48c0b",
            "8b0c13fc7fd64c169e2598f23d7d84ba"
          ]
        },
        "outputId": "dd31e776-1ea4-4419-db07-12bb25ac748e"
      },
      "source": [
        "trainloader = CIFAR100DataLoader('train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17a5d13a770b4334865005366206befd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X59N-5lXDcwt"
      },
      "source": [
        "def imshow(img):\n",
        "    #img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    #plt.imshow(npimg)\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5268MI89_6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "6a7b9f92-9492-4b94-a60c-9e0fe6cf389c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "trainiter = iter(trainloader)\n",
        "features, labels = next(trainiter)\n",
        "features.shape, labels.shape\n",
        "imshow(torchvision.utils.make_grid(features[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa8mOWiGFbEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d85fca-c29a-4559-cd99-e7de9148211e"
      },
      "source": [
        "#model = VisionTransformer(patch_size=patch_size, max_len=max_len, embed_dim=embed_dim, classes=classes, layers=layers, heads=heads).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.AdamW(classhead.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(trainloader), epochs=epochs)\n",
        "\n",
        "#optimizer = torch.optim.SGD(classhead.parameters(),\n",
        "                       # lr=lr,\n",
        "                       #         momentum=0.9,\n",
        "                        #        weight_decay=weight_decay)\n",
        "#scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=t_total)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for data, target in tqdm(trainloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        trans_output = model(data).to(device)\n",
        "        output = classhead(trans_output[:,0])\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(trainloader)\n",
        "        running_loss += loss.item() / len(trainloader)\n",
        "    \n",
        "    print(f\"Epoch : {epoch+1} - loss : {running_loss:.4f} - acc: {running_accuracy:.4f}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [30:22<00:00,  2.33s/it]\n",
            "  0%|          | 0/782 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1 - loss : 4.5982 - acc: 0.3540\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|â–ˆâ–ˆâ–ˆ       | 236/782 [09:13<21:11,  2.33s/it]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ian_IIUgtfEG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}