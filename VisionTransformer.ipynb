{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOLKY+RozHbNnBs+6eHcLEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, heads=8, embed_dim=512, activation='relu', dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        assert inp.size(2) == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "        \n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOGwfjHJQR_M",
        "outputId": "bdac5073-e367-4b5e-aa80-5124ae3596fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "attention = Attention(embed_dim=4, heads=2, activation=None)\n",
        "attention_lib = nn.MultiheadAttention(embed_dim=4, num_heads=2)\n",
        "inp = torch.ones((1, 2, 4))\n",
        "print(inp)\n",
        "out, wts = attention(inp)\n",
        "print(out)\n",
        "print(wts)\n",
        "out_lib, out_wts = attention_lib(inp.permute(1, 0, 2), inp.permute(1, 0, 2), inp.permute(1, 0, 2))\n",
        "print(out_lib.permute(1, 0, 2))\n",
        "print(out_wts)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[-2.1490,  0.8508,  0.5317, -0.9154],\n",
            "         [-2.1490,  0.8508,  0.5317, -0.9154]]], grad_fn=<UnsafeViewBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]],\n",
            "\n",
            "        [[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[[-0.6255,  0.0154,  0.3488,  0.4291],\n",
            "         [-0.6255,  0.0154,  0.3488,  0.4291]]], grad_fn=<PermuteBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}