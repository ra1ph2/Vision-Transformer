{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VisionTransformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNaSgVApF+X/PfAipdXmWrs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ra1ph2/Vision-Transformer/blob/main/VisionTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7v2dPqfZFIV"
      },
      "source": [
        "#### Libraries Imported"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bQIxfD9Ep0y"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import functional as F\n",
        "from torch import optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS7R221iUMI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f64c389c-e636-4a0d-d0d1-b3c33fbc4abc"
      },
      "source": [
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU: ', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU:  Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFc5EfTgZIBk"
      },
      "source": [
        "#### Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_75W0devEPY7"
      },
      "source": [
        "##### Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idmPOFlDIGDZ"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, dropout=0.1):\n",
        "        super(Attention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        else:\n",
        "            self.activation = nn.Identity()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        query = self.activation(self.query(inp))\n",
        "        key   = self.activation(self.key(inp))\n",
        "        value = self.activation(self.value(inp))\n",
        "\n",
        "        # output of _reshape_heads(): (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        query = self._reshape_heads(query)\n",
        "        key   = self._reshape_heads(key)\n",
        "        value = self._reshape_heads(value)\n",
        "\n",
        "        # attention_scores: (batch_size * heads, seq_len, seq_len) | Softmaxed along the last dimension\n",
        "        attention_scores = self.softmax(torch.matmul(query, key.transpose(1, 2)))\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        out = torch.matmul(self.dropout(attention_scores), value)\n",
        "        \n",
        "        # output of _reshape_heads_back(): (batch_size, seq_len, embed_size)\n",
        "        out = self._reshape_heads_back(out)\n",
        "\n",
        "        return out, attention_scores\n",
        "\n",
        "    def _reshape_heads(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "\n",
        "        reduced_dim = self.embed_dim // self.heads\n",
        "        assert reduced_dim * self.heads == self.embed_dim\n",
        "        out = inp.reshape(batch_size, seq_len, self.heads, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(-1, seq_len, reduced_dim)\n",
        "\n",
        "        # out: (batch_size * heads, seq_len, reduced_dim)\n",
        "        return out\n",
        "\n",
        "    def _reshape_heads_back(self, inp):\n",
        "        # inp: (batch_size * heads, seq_len, reduced_dim) | reduced_dim = embed_dim // heads\n",
        "        batch_size_mul_heads, seq_len, reduced_dim = inp.size()\n",
        "        batch_size = batch_size_mul_heads // self.heads\n",
        "\n",
        "        out = inp.reshape(batch_size, self.heads, seq_len, reduced_dim)\n",
        "        out = out.permute(0, 2, 1, 3)\n",
        "        out = out.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOGwfjHJQR_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05d8173b-d82b-4577-ee1c-607a7a2de088"
      },
      "source": [
        "attention = Attention(embed_dim=4, heads=2, activation=None)\n",
        "attention_lib = nn.MultiheadAttention(embed_dim=4, num_heads=2)\n",
        "inp = torch.ones((1, 2, 4))\n",
        "print(inp)\n",
        "out, wts = attention(inp)\n",
        "print(out)\n",
        "print(wts)\n",
        "out_lib, out_wts = attention_lib(inp.permute(1, 0, 2), inp.permute(1, 0, 2), inp.permute(1, 0, 2))\n",
        "print(out_lib.permute(1, 0, 2))\n",
        "print(out_wts)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1.]]])\n",
            "tensor([[[-1.1731, -0.7017,  0.1789, -0.4591],\n",
            "         [-1.1731, -0.7017,  0.3579, -0.9181]]], grad_fn=<UnsafeViewBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]],\n",
            "\n",
            "        [[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<SoftmaxBackward>)\n",
            "tensor([[[ 0.4169, -0.3535,  0.5380, -0.0048],\n",
            "         [ 0.4169, -0.3535,  0.5380, -0.0048]]], grad_fn=<PermuteBackward>)\n",
            "tensor([[[0.5000, 0.5000],\n",
            "         [0.5000, 0.5000]]], grad_fn=<DivBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYemII1gElcK"
      },
      "source": [
        "# Check if Dropout should be used after second Linear Layer\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, forward_expansion=1, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim * forward_expansion)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim * forward_expansion, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.dropout(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1r7Cak2F9CN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f1ba6ba-7986-4975-cabd-99d83369ffff"
      },
      "source": [
        "ff = FeedForward(8, 1)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = ff(inp)\n",
        "print(out)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[-0.4022, -0.1908, -0.2099, -0.7377, -0.4824, -0.1971,  0.4107,\n",
            "          -0.0838],\n",
            "         [-0.3396, -0.2296, -0.1666, -0.8117, -0.4957, -0.2519,  0.3794,\n",
            "          -0.0240]]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMrCi7DNGvCc"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = Attention(embed_dim, heads, activation, dropout)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = FeedForward(embed_dim, forward_expansion, dropout)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "        batch_size, seq_len, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        res = inp\n",
        "        out = self.norm1(inp)\n",
        "        out, _ = self.attention(out)\n",
        "        out = out + res\n",
        "        \n",
        "        res = out\n",
        "        out = self.norm2(out)\n",
        "        out = self.feed_forward(out)\n",
        "        out = out + res\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IomPGN9UJj1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6755ce21-7707-439d-8205-60f3135f8c9c"
      },
      "source": [
        "tb = TransformerBlock(8, 2)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = tb(inp)\n",
        "print(out)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[1.1547, 0.8325, 0.6509, 1.4753, 1.3749, 0.7332, 1.1896, 1.3985],\n",
            "         [0.9682, 1.0148, 0.4637, 1.6533, 1.6283, 0.7369, 1.3886, 1.4294]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcBcReoPJ4NC"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, embed_dim, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trans_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_dim, heads, activation, forward_expansion, dropout) for i in range(layers)]\n",
        "        )\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        out = inp\n",
        "        for block in self.trans_blocks:\n",
        "            out = block(out)\n",
        "\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfxmE-mJLILR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c44c23-be73-45a5-eb6a-7608692c31d4"
      },
      "source": [
        "tb = TransformerBlock(8, 2)\n",
        "trans = Transformer(8, 1, 2)\n",
        "inp = torch.ones((1, 2, 8))\n",
        "print(inp)\n",
        "out = tb(inp)\n",
        "print(out)\n",
        "out = trans(inp)\n",
        "print(out)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "         [1., 1., 1., 1., 1., 1., 1., 1.]]])\n",
            "tensor([[[0.8487, 1.2963, 1.6188, 0.7900, 1.2649, 1.2814, 0.6702, 0.9457],\n",
            "         [0.8910, 1.2740, 1.5701, 0.6446, 1.3154, 1.2624, 0.6804, 0.9917]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "tensor([[[1.2206, 0.9696, 1.1893, 0.7922, 1.3784, 1.3628, 0.5589, 0.4503],\n",
            "         [1.3160, 0.7839, 1.0892, 0.7954, 1.6322, 1.1919, 0.3686, 0.4362]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRN2k5-kLWYG"
      },
      "source": [
        "# Not Exactly Same as Paper | Check if Dropout should be used here\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, embed_dim, classes, dropout=0.1):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.classes = classes\n",
        "        self.fc1 = nn.Linear(embed_dim, embed_dim // 2)\n",
        "        self.activation = nn.GELU()\n",
        "        self.fc2 = nn.Linear(embed_dim // 2, classes)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, embed_dim)\n",
        "        batch_size, embed_dim = inp.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "\n",
        "        out = self.dropout(self.activation(self.fc1(inp)))\n",
        "        # out = self.softmax(self.fc2(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        # out: (batch_size, embed_dim) | SoftMaxed along the last dimension\n",
        "        return out"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfBFnhBhOFIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cd5d0d1-5f4f-4729-e79c-1eb77faa226f"
      },
      "source": [
        "ch = ClassificationHead(8, 2)\n",
        "inp = torch.ones((2, 8))\n",
        "print(inp)\n",
        "out = ch(inp)\n",
        "print(out)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[-0.2365, -0.4102],\n",
            "        [-0.2365, -0.4102]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-8nXI7gPxIs"
      },
      "source": [
        "#TODO: Remove to.device from class_token\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, patch_size, max_len, embed_dim, classes, layers, heads=8, activation=None, forward_expansion=1, dropout=0.1):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.name = 'VisionTransformer'\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_to_embed = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.position_embed = nn.Parameter(torch.randn((max_len, embed_dim)))\n",
        "        self.transformer = Transformer(embed_dim, layers, heads, activation, forward_expansion, dropout)\n",
        "        self.classification_head = ClassificationHead(embed_dim, classes)\n",
        "        self.class_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, 3, width, height)\n",
        "        batch_size, channels, width, height = inp.size()\n",
        "        assert channels == 3\n",
        "\n",
        "        out = inp.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size).contiguous()\n",
        "        out = out.view(batch_size, channels, -1, self.patch_size, self.patch_size)\n",
        "        out = out.permute(0, 2, 3, 4, 1)\n",
        "        # out: (batch_size, seq_len, patch_size, patch_size, 3) | seq_len would be (width*height)/(patch_size**2)\n",
        "        batch_size, seq_len, patch_size, _, channels = out.size()\n",
        "        \n",
        "        out = out.reshape(batch_size, seq_len, -1)\n",
        "        out = self.patch_to_embed(out)\n",
        "        # out: (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        class_token = self.class_token.expand(batch_size, -1, -1)\n",
        "        out = torch.cat([class_token, out], dim=1)\n",
        "        # out: (batch_size, seq_len+1, embed_dim)\n",
        "\n",
        "        position_embed = self.position_embed[:seq_len+1]\n",
        "        position_embed = position_embed.unsqueeze(0).expand(batch_size, seq_len+1, self.embed_dim)\n",
        "        out = out + position_embed\n",
        "        # out: (batch_size, seq_len+1, embed_dim) | Added Positional Embeddings\n",
        "\n",
        "        out = self.transformer(out)\n",
        "        # out: (batch_size, seq_len+1, embed_dim) \n",
        "        class_token = out[:, 0]\n",
        "        # class_token: (batch_size, embed_dim)\n",
        "\n",
        "        class_out = self.classification_head(class_token)\n",
        "        # class_out: (batch_size, classes)\n",
        "        \n",
        "        return class_out, out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W82aFCPPWIus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51ad900e-cafb-44d1-b729-9c09ba1dad8a"
      },
      "source": [
        "vit = VisionTransformer(2, 3, 8, 2, 2).to(device)\n",
        "inp = torch.ones((2, 3, 2, 2)).to(device)\n",
        "print(inp)\n",
        "class_out, out = vit(inp)\n",
        "print(class_out)\n",
        "print(class_out.shape)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]],\n",
            "\n",
            "         [[1., 1.],\n",
            "          [1., 1.]]]], device='cuda:0')\n",
            "tensor([[-0.3383,  0.5653],\n",
            "        [-0.1804,  0.3965]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "torch.Size([2, 2])\n",
            "tensor([[[ 0.7487, -0.9380,  2.1415,  1.6897, -0.3489,  2.0259,  0.9920,\n",
            "           2.5599],\n",
            "         [ 0.1592, -0.4236, -0.2012,  1.2172, -1.0030,  1.0711,  1.0290,\n",
            "           1.9629]],\n",
            "\n",
            "        [[ 0.7187, -1.0949,  1.9959,  0.7508,  0.1739,  1.9024,  0.7963,\n",
            "           1.4988],\n",
            "         [-0.4558, -0.9345, -0.1858,  1.1070, -0.6087,  0.5529,  0.5941,\n",
            "           1.8732]]], device='cuda:0', grad_fn=<AddBackward0>)\n",
            "torch.Size([2, 2, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsEV_yqAETZA"
      },
      "source": [
        "##### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQxpId2jEV_R"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_channels, out_channels, residual_downsample=None, stride=1, expansion=4):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.expansion = expansion\n",
        "        self.conv1 = nn.Conv2d(input_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.bnorm3 = nn.BatchNorm2d(out_channels * expansion)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.residual_downsample = residual_downsample\n",
        "    \n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        res = inp\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.activation(self.bnorm2(self.conv2(out)))\n",
        "        out = self.activation(self.bnorm3(self.conv3(out)))\n",
        "        \n",
        "        if self.residual_downsample is not None:\n",
        "            res = self.residual_downsample(res)\n",
        "\n",
        "        out = self.activation(out + res)\n",
        "\n",
        "        # out: (batch_size, out_channels * expansion, height, width) | height, width depending on stride\n",
        "        return out"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ss--kZkSGNU"
      },
      "source": [
        "class ResNetLarge(nn.Module):\n",
        "    def __init__(self, layers, input_channels, classes):\n",
        "        super(ResNetLarge, self).__init__()\n",
        "        self.name = 'ResNet'\n",
        "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bnorm1 = nn.BatchNorm2d(64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._layer(layers[0], input_channels=64, output_channels=64, stride=1)\n",
        "        self.layer2 = self._layer(layers[1], input_channels=256, output_channels=128, stride=2)\n",
        "        self.layer3 = self._layer(layers[2], input_channels=512, output_channels=256, stride=2)\n",
        "        self.layer4 = self._layer(layers[3], input_channels=1024, output_channels=512, stride=2)\n",
        "\n",
        "        self.avppool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, classes)\n",
        "\n",
        "    def forward(self, inp):\n",
        "        # inp: (batch_size, input_channels, height, width)\n",
        "\n",
        "        out = self.activation(self.bnorm1(self.conv1(inp)))\n",
        "        out = self.maxpool(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        out = self.avppool(out)\n",
        "        out = out.reshape(out.shape[0], -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # out: (batch_size, classes)\n",
        "        return out\n",
        "\n",
        "    def _layer(self, num_layers, input_channels, output_channels, stride):\n",
        "        residual_downsample = None\n",
        "        layers = []\n",
        "\n",
        "        # 4 is the value of the expansion for large ResNets\n",
        "        if stride != 1 or input_channels != output_channels * 4:\n",
        "            residual_downsample = nn.Sequential(\n",
        "                nn.Conv2d(input_channels, output_channels * 4, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(output_channels * 4)\n",
        "            )\n",
        "        \n",
        "        layers.append(ResidualBlock(input_channels, output_channels, residual_downsample, stride))\n",
        "\n",
        "        for i in range(num_layers - 1):\n",
        "            layers.append(ResidualBlock(output_channels * 4, output_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeceF_RUaVO4"
      },
      "source": [
        "def ResNet50(input_channels, classes):\n",
        "    return ResNetLarge([3, 4, 6, 3], input_channels, classes)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGS4bmCRWmIg"
      },
      "source": [
        "#### Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60fXexWWWlpJ"
      },
      "source": [
        "def CIFAR100DataLoader(split, batch_size=8, num_workers=2, shuffle=True):\n",
        "\n",
        "    CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
        "    CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
        "    \n",
        "    mean = CIFAR100_TRAIN_MEAN\n",
        "    std = CIFAR100_TRAIN_STD\n",
        "\n",
        "    if split == 'train':\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop((224,224), scale=(0.5, 1.0)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        # train_transform = transforms.Compose([\n",
        "        #     transforms.RandomCrop(32, padding=4),\n",
        "        #     transforms.RandomHorizontalFlip(),\n",
        "        #     transforms.RandomRotation(15),\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Normalize(mean, std)\n",
        "        # ])\n",
        "        \n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "    \n",
        "    elif split == 'test':\n",
        "        test_transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])\n",
        "        ])\n",
        "\n",
        "        # test_transform = transforms.Compose([\n",
        "        #     transforms.ToTensor(),\n",
        "        #     transforms.Normalize(mean, std)\n",
        "        # ])\n",
        "\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "        dataloader = DataLoader(cifar100, batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "\n",
        "    return dataloader"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB_YjSG0aLJR"
      },
      "source": [
        "#### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Byfz7zrEaKxq"
      },
      "source": [
        "lr = 0.001\n",
        "batch_size = 128\n",
        "num_workers = 2\n",
        "shuffle = True\n",
        "patch_size = 16\n",
        "image_sz = 224\n",
        "max_len = ((image_sz//patch_size) * (image_sz//patch_size)) + 1 # +1 for the class token\n",
        "embed_dim = 512\n",
        "classes = 100\n",
        "layers = 6\n",
        "heads = 8\n",
        "epochs = 100"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89pJGwZaUBs"
      },
      "source": [
        "def train(model, dataloader, criterion, optimizer, scheduler):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for data, target in tqdm(dataloader):\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        if model.name == 'VisionTransformer':\n",
        "            output, _ = model(data)\n",
        "        elif model.name == 'ResNet':\n",
        "            output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        acc = (output.argmax(dim=1) == target).float().mean()\n",
        "        running_accuracy += acc / len(dataloader)\n",
        "        running_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return running_loss, running_accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXAFKy14EIyU"
      },
      "source": [
        "def evaluation(model, dataloader, criterion):\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_accuracy = 0.0\n",
        "        test_loss = 0.0\n",
        "        for data, target in tqdm(dataloader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            if model.name == 'VisionTransformer':\n",
        "                output, _ = model(data)\n",
        "            elif model.name == 'ResNet':\n",
        "                output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            acc = (output.argmax(dim=1) == target).float().mean()\n",
        "            test_accuracy += acc / len(dataloader)\n",
        "            test_loss += loss.item() / len(dataloader)\n",
        "\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofAkCvMzfN3R"
      },
      "source": [
        "##### Model - Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG_wT7pXe5q9"
      },
      "source": [
        "model = VisionTransformer(\n",
        "    patch_size=patch_size,\n",
        "    max_len=max_len,\n",
        "    embed_dim=embed_dim,\n",
        "    classes=classes,\n",
        "    layers=layers,\n",
        "    heads=heads).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq_fd2gjfRqD"
      },
      "source": [
        "##### Model - ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztkBOLRe6pO"
      },
      "source": [
        "model = ResNet50(\n",
        "    input_channels=3,\n",
        "    classes=classes).to(device)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0x9qE4HhUhR"
      },
      "source": [
        "##### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQel00aDWi4r",
        "outputId": "cdf32c08-9f47-4690-cf77-27c9b69e263c"
      },
      "source": [
        "train_dataloader = CIFAR100DataLoader(split='train', batch_size=batch_size, num_workers=num_workers, shuffle=shuffle)\n",
        "test_dataloader = CIFAR100DataLoader(split='test', batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_dataloader), epochs=epochs)\n",
        "\n",
        "training_accs = []\n",
        "test_accs = []\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss, running_accuracy = train(model, train_dataloader, criterion, optimizer, scheduler)\n",
        "    print(f\"Epoch : {epoch+1} - acc: {running_accuracy:.4f} - loss : {running_loss:.4f}\\n\")\n",
        "    training_accs.append(running_accuracy)\n",
        "\n",
        "    test_loss, test_accuracy = test(model, test_dataloader, criterion)\n",
        "    print(f\"test acc: {test_accuracy:.4f} - test loss : {test_loss:.4f}\\n\")\n",
        "    test_accs.append(test_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▋         | 25/391 [00:32<07:39,  1.26s/it]"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}